{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5957fb9-97b1-465d-8951-20ca2b0be560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-api-python-client\n",
      "  Using cached google_api_python_client-2.187.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.12/site-packages (1.2.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.2.3)\n",
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /opt/conda/lib/python3.12/site-packages (from google-api-python-client) (0.31.0)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 (from google-api-python-client)\n",
      "  Using cached google_auth-2.43.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client)\n",
      "  Using cached google_auth_httplib2-0.2.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 (from google-api-python-client)\n",
      "  Using cached google_api_core-2.28.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /opt/conda/lib/python3.12/site-packages (from google-api-python-client) (4.2.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.12/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.12/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client)\n",
      "  Using cached googleapis_common_protos-1.72.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /opt/conda/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (5.29.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/conda/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /opt/conda/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.32.3)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9.1)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in /home/jovyan/.local/lib/python3.12/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /opt/conda/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.6.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2024.12.14)\n",
      "Using cached google_api_python_client-2.187.0-py3-none-any.whl (14.6 MB)\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Using cached google_api_core-2.28.1-py3-none-any.whl (173 kB)\n",
      "Using cached google_auth-2.43.0-py2.py3-none-any.whl (223 kB)\n",
      "Using cached google_auth_httplib2-0.2.1-py3-none-any.whl (9.5 kB)\n",
      "Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Using cached googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "Installing collected packages: googleapis-common-protos, et-xmlfile, openpyxl, google-auth, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "Successfully installed et-xmlfile-2.0.0 google-api-core-2.28.1 google-api-python-client-2.187.0 google-auth-2.43.0 google-auth-httplib2-0.2.1 googleapis-common-protos-1.72.0 openpyxl-3.1.5\n"
     ]
    }
   ],
   "source": [
    "# I installed the needed libraries only for my user account.\n",
    "# This avoids the \"Permission denied\" error.\n",
    "\n",
    "!pip install --user google-api-python-client python-dotenv pandas openpyxl nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91e9ca5b-24e0-4465-929d-406d4d203b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IST652 Mini Project â€“ F1 YouTube Comments + Kaggle Race Data\n",
    "# ============================================================\n",
    "\n",
    "# Installing packages\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1990c023-1bad-47e1-a6bb-e0a5539c5d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------\n",
    "# 0. Setup: NLTK + API key\n",
    "# --------------------------\n",
    "\n",
    "load_dotenv()\n",
    "nltk.download(\"vader_lexicon\", quiet=True)\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# YouTube API key:\n",
    "YOUTUBE_API_KEY = \"YOUTUBE_API_KEY\"\n",
    "\n",
    "if not YOUTUBE_API_KEY or YOUTUBE_API_KEY == \"PASTE_YOUR_REAL_API_KEY_HERE\":\n",
    "    raise ValueError(\"Please paste your actual YouTube API key in YOUTUBE_API_KEY.\")\n",
    "\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=YOUTUBE_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e45ec04-c905-4322-abd6-c4bbd03302da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching comments for video: Jt3gVEtfy1c\n",
      "Fetching comments for video: 2RKcJhnhtlU\n",
      "Fetching comments for video: Cb9Wo7sPGVg\n",
      "Fetching comments for video: 1E1-_beyrNQ\n",
      "Fetching comments for video: pcmZp9kMVDg\n",
      "Fetching comments for video: cqJBQ7bx1L4\n",
      "Fetching comments for video: -o7fAgMnOZA\n",
      "Fetching comments for video: bijI5emoGgg\n",
      "Fetching comments for video: MK83clSv6-k\n",
      "Fetching comments for video: oavaWsg56d4\n",
      "Fetching comments for video: Sy8Sa9_2yXw\n",
      "Fetching comments for video: WgBuHqqE7Mw\n",
      "Fetching comments for video: VJuKqOQBDhs\n",
      "Fetching comments for video: eeS1_6UF1CI\n",
      "Fetching comments for video: 4Z3y6mYDJgM\n",
      "Fetching comments for video: 683MegAx5F4\n",
      "Fetching comments for video: MYpwaMHYDfw\n",
      "Fetching comments for video: 0JIQVXsSzps\n",
      "Fetching comments for video: ZYwWyNnLKa4\n",
      "Fetching comments for video: 55_LalL3E6Y\n",
      "Fetching comments for video: -WoFoUPDCow\n",
      "Fetching comments for video: -dCLabtcZBQ\n",
      "Fetching comments for video: lWY2naac5f4\n",
      "Fetching comments for video: iz4TxOSy7Rs\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# 1. ting data from YouTube (API source)\n",
    "# =====================================\n",
    "\n",
    "F1_CHANNEL_ID = \"UCB_qr75-ydFVKSF9Dmo6izg\"  # Formula 1 official channel\n",
    "\n",
    "\n",
    "def get_race_highlight_videos(youtube_client, max_videos=12):\n",
    "    \"\"\"\n",
    "    Search the Formula 1 channel for 'Race Highlights' videos\n",
    "    and return a dataframe with videoId, title, and publishedAt.\n",
    "    \"\"\"\n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(videos) < max_videos:\n",
    "        request = youtube_client.search().list(\n",
    "            part=\"snippet\",\n",
    "            channelId=F1_CHANNEL_ID,\n",
    "            q=\"Race Highlights\",\n",
    "            type=\"video\",\n",
    "            order=\"date\",\n",
    "            maxResults=min(50, max_videos - len(videos)),\n",
    "            pageToken=next_page_token,\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response.get(\"items\", []):\n",
    "            videos.append(\n",
    "                {\n",
    "                    \"videoId\": item[\"id\"][\"videoId\"],\n",
    "                    \"title\": item[\"snippet\"][\"title\"],\n",
    "                    \"publishedAt\": item[\"snippet\"][\"publishedAt\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(videos)\n",
    "\n",
    "\n",
    "def get_video_stats(youtube_client, video_ids):\n",
    "    \"\"\"\n",
    "    For a list of video IDs, fetch statistics and basic info:\n",
    "    views, likes, comments, and duration.\n",
    "    \"\"\"\n",
    "    all_stats = []\n",
    "\n",
    "    for i in range(0, len(video_ids), 50):  # API allows up to 50 IDs per call\n",
    "        chunk = video_ids[i : i + 50]\n",
    "        request = youtube_client.videos().list(\n",
    "            part=\"snippet,contentDetails,statistics\", id=\",\".join(chunk)\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response.get(\"items\", []):\n",
    "            stats = item.get(\"statistics\", {})\n",
    "            snippet = item.get(\"snippet\", {})\n",
    "            content = item.get(\"contentDetails\", {})\n",
    "\n",
    "            all_stats.append(\n",
    "                {\n",
    "                    \"videoId\": item[\"id\"],\n",
    "                    \"title\": snippet.get(\"title\"),\n",
    "                    \"publishedAt\": snippet.get(\"publishedAt\"),\n",
    "                    \"duration\": content.get(\"duration\"),\n",
    "                    \"viewCount\": int(stats.get(\"viewCount\", 0)),\n",
    "                    \"likeCount\": int(stats.get(\"likeCount\", 0)),\n",
    "                    \"commentCount\": int(stats.get(\"commentCount\", 0)),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(all_stats)\n",
    "\n",
    "\n",
    "def get_comments_for_video(\n",
    "    youtube_client, video_id, max_comments=200, sleep_seconds=0.2\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch up to max_comments top-level comments for one video.\n",
    "    If comments are disabled, skip the video and return an empty list.\n",
    "    \"\"\"\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(comments) < max_comments:\n",
    "        try:\n",
    "            request = youtube_client.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=min(100, max_comments - len(comments)),\n",
    "                textFormat=\"plainText\",\n",
    "                pageToken=next_page_token,\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except HttpError as e:\n",
    "            # Comments disabled for this video\n",
    "            if e.resp.status == 403 and \"commentsDisabled\" in str(e):\n",
    "                print(f\"Comments are disabled for video {video_id}. Skipping.\")\n",
    "                return []\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        for item in response.get(\"items\", []):\n",
    "            top_comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "            comments.append(\n",
    "                {\n",
    "                    \"videoId\": video_id,\n",
    "                    \"commentId\": item[\"snippet\"][\"topLevelComment\"][\"id\"],\n",
    "                    \"author\": top_comment.get(\"authorDisplayName\"),\n",
    "                    \"text\": top_comment.get(\"textDisplay\"),\n",
    "                    \"likeCount\": int(top_comment.get(\"likeCount\", 0)),\n",
    "                    \"publishedAt\": top_comment.get(\"publishedAt\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "        time.sleep(sleep_seconds)\n",
    "\n",
    "    return comments\n",
    "\n",
    "\n",
    "def get_comments_for_videos(youtube_client, video_ids, max_comments_per_video=150):\n",
    "    \"\"\"\n",
    "    Loop through each video ID and collect comments into a single dataframe.\n",
    "    \"\"\"\n",
    "    all_comments = []\n",
    "    for vid in video_ids:\n",
    "        print(f\"Fetching comments for video: {vid}\")\n",
    "        video_comments = get_comments_for_video(\n",
    "            youtube_client, vid, max_comments=max_comments_per_video\n",
    "        )\n",
    "        all_comments.extend(video_comments)\n",
    "    return pd.DataFrame(all_comments)\n",
    "\n",
    "\n",
    "# --- 1A. Getting videos and stats from YouTube ---\n",
    "\n",
    "videos_basic_df = get_race_highlight_videos(youtube, max_videos=12)\n",
    "videos_df = get_video_stats(youtube, videos_basic_df[\"videoId\"].tolist())\n",
    "\n",
    "# Only keeping videos that have comments\n",
    "videos_with_comments_df = videos_df[videos_df[\"commentCount\"] > 0].copy()\n",
    "video_ids_for_comments = videos_with_comments_df[\"videoId\"].tolist()\n",
    "\n",
    "# --- 1B. Getting comments for those videos ---\n",
    "\n",
    "comments_df = get_comments_for_videos(\n",
    "    youtube, video_ids_for_comments, max_comments_per_video=150\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eaf39e5d-8263-440c-8564-558e3fc44fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# 2. Getting data from Kaggle (CSV / second source)\n",
    "# ===============================================\n",
    "\n",
    "# Race_Schedule.csv comes from Kaggle.\n",
    "race_schedule_df = pd.read_csv(\"Race_Schedule.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4d707d5-66c2-46ae-99a4-0c770d7b010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3. Combining YouTube + Kaggle race schedule\n",
    "# ============================================\n",
    "\n",
    "def extract_year_and_race(title):\n",
    "    \"\"\"\n",
    "    From a title like '2024 Bahrain Grand Prix | Race Highlights',\n",
    "    extract race_year = 2024 and race_name = 'Bahrain Grand Prix'.\n",
    "    \"\"\"\n",
    "    if not isinstance(title, str):\n",
    "        return None, None\n",
    "\n",
    "    year_match = re.search(r\"(20\\d{2})\", title)\n",
    "    year = int(year_match.group(1)) if year_match else None\n",
    "\n",
    "    gp_match = re.search(r\"([A-Za-z\\s]+Grand Prix)\", title)\n",
    "    race_name = gp_match.group(1).strip() if gp_match else None\n",
    "\n",
    "    return year, race_name\n",
    "\n",
    "\n",
    "def normalize_name(name):\n",
    "    \"\"\"\n",
    "    Normalize race names:\n",
    "    - cast to string\n",
    "    - lowercase\n",
    "    - collapse extra spaces\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        return None\n",
    "    name = str(name)\n",
    "    name = name.lower()\n",
    "    name = re.sub(r\"\\s+\", \" \", name).strip()\n",
    "    return name\n",
    "\n",
    "\n",
    "# --- 3A. Extracting year and race name from video titles ---\n",
    "\n",
    "videos_df[[\"race_year\", \"race_name_raw\"]] = videos_df[\"title\"].apply(\n",
    "    lambda t: pd.Series(extract_year_and_race(t))\n",
    ")\n",
    "videos_df[\"race_name_norm\"] = videos_df[\"race_name_raw\"].apply(normalize_name)\n",
    "\n",
    "# --- 3B. Preparing race schedule columns for join ---\n",
    "\n",
    "cols_lower = [c.lower() for c in race_schedule_df.columns]\n",
    "\n",
    "if \"season\" in cols_lower:\n",
    "    year_col = race_schedule_df.columns[cols_lower.index(\"season\")]\n",
    "elif \"year\" in cols_lower:\n",
    "    year_col = race_schedule_df.columns[cols_lower.index(\"year\")]\n",
    "else:\n",
    "    year_col = race_schedule_df.columns[0]\n",
    "\n",
    "race_name_col = None\n",
    "for col in race_schedule_df.columns:\n",
    "    if \"grand\" in col.lower() or \"race\" in col.lower() or \"gp\" in col.lower():\n",
    "        race_name_col = col\n",
    "        break\n",
    "if race_name_col is None:\n",
    "    race_name_col = race_schedule_df.columns[1]\n",
    "\n",
    "race_schedule_df[\"race_year\"] = race_schedule_df[year_col]\n",
    "race_schedule_df[\"race_name_raw\"] = race_schedule_df[race_name_col]\n",
    "race_schedule_df[\"race_name_norm\"] = race_schedule_df[\"race_name_raw\"].apply(\n",
    "    normalize_name\n",
    ")\n",
    "\n",
    "race_schedule_join = race_schedule_df[[\"race_year\", \"race_name_norm\"]].drop_duplicates()\n",
    "\n",
    "# --- 3C. Joining YouTube videos with race schedule ---\n",
    "\n",
    "videos_merged_df = pd.merge(\n",
    "    videos_df,\n",
    "    race_schedule_join,\n",
    "    on=[\"race_year\", \"race_name_norm\"],\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Also attaching race info to comments via videoId\n",
    "comments_combined_df = comments_df.merge(\n",
    "    videos_merged_df[[\"videoId\", \"race_year\", \"race_name_norm\"]],\n",
    "    on=\"videoId\",\n",
    "    how=\"left\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fcc8f0d-8d84-4a20-aadc-b219ff98504b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete. CSV files created:\n",
      " - output_f1_video_summary.csv\n",
      " - output_f1_driver_mentions.csv\n",
      " - output_f1_driver_sentiment_engagement.csv\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 4. Analysis + Output files\n",
    "# ==============================\n",
    "\n",
    "# ---- 4.1 Analysis 1: Video-level engagement summary ----\n",
    "\n",
    "q1_df = videos_merged_df.copy()\n",
    "q1_df[\"like_view_ratio\"] = q1_df[\"likeCount\"] / q1_df[\"viewCount\"].replace(0, np.nan)\n",
    "q1_df[\"comment_view_ratio\"] = q1_df[\"commentCount\"] / q1_df[\"viewCount\"].replace(\n",
    "    0, np.nan\n",
    ")\n",
    "\n",
    "q1_df_out = q1_df[\n",
    "    [\n",
    "        \"videoId\",\n",
    "        \"title\",\n",
    "        \"race_year\",\n",
    "        \"race_name_raw\",\n",
    "        \"viewCount\",\n",
    "        \"likeCount\",\n",
    "        \"commentCount\",\n",
    "        \"like_view_ratio\",\n",
    "        \"comment_view_ratio\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "q1_df_out.to_csv(\"output_f1_video_summary.csv\", index=False)\n",
    "\n",
    "\n",
    "# ---- 4.2 Adding driver mentions + sentiment to comments ----\n",
    "\n",
    "drivers_list = [\n",
    "    \"Verstappen\",\n",
    "    \"Hamilton\",\n",
    "    \"Russell\",\n",
    "    \"Norris\",\n",
    "    \"Leclerc\",\n",
    "    \"Sainz\",\n",
    "    \"Perez\",\n",
    "    \"Piastri\",\n",
    "    \"Alonso\",\n",
    "    \"Tsunoda\",\n",
    "    \"Stroll\",\n",
    "    \"Gasly\",\n",
    "    \"Ocon\",\n",
    "    \"Hulkenberg\",\n",
    "    \"Bottas\",\n",
    "    \"Zhou\",\n",
    "    \"Ricciardo\",\n",
    "    \"Magnussen\",\n",
    "]\n",
    "\n",
    "\n",
    "def find_drivers_in_text(text, drivers):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    found = []\n",
    "    for d in drivers:\n",
    "        if re.search(r\"\\b\" + re.escape(d) + r\"\\b\", str(text), flags=re.IGNORECASE):\n",
    "            found.append(d)\n",
    "    return list(set(found))\n",
    "\n",
    "\n",
    "def get_sentiment_label(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return 0.0, \"neutral\"\n",
    "    scores = sia.polarity_scores(text)\n",
    "    comp = scores[\"compound\"]\n",
    "    if comp >= 0.05:\n",
    "        label = \"positive\"\n",
    "    elif comp <= -0.05:\n",
    "        label = \"negative\"\n",
    "    else:\n",
    "        label = \"neutral\"\n",
    "    return comp, label\n",
    "\n",
    "\n",
    "comments_combined_df[\"drivers_mentioned\"] = comments_combined_df[\"text\"].apply(\n",
    "    lambda t: find_drivers_in_text(t, drivers_list)\n",
    ")\n",
    "comments_combined_df[[\"sentiment_score\", \"sentiment_label\"]] = comments_combined_df[\n",
    "    \"text\"\n",
    "].apply(lambda t: pd.Series(get_sentiment_label(t)))\n",
    "\n",
    "\n",
    "# ---- 4.3 Analysis 2: Driver mentions & comment likes ----\n",
    "\n",
    "comments_exploded = comments_combined_df.explode(\"drivers_mentioned\")\n",
    "comments_exploded = comments_exploded[\n",
    "    ~comments_exploded[\"drivers_mentioned\"].isna()\n",
    "]\n",
    "\n",
    "driver_stats = (\n",
    "    comments_exploded.groupby(\"drivers_mentioned\")\n",
    "    .agg(\n",
    "        total_mentions=(\"commentId\", \"count\"),\n",
    "        total_comment_likes=(\"likeCount\", \"sum\"),\n",
    "        avg_comment_likes=(\"likeCount\", \"mean\"),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"drivers_mentioned\": \"driver_name\"})\n",
    ")\n",
    "\n",
    "driver_stats = driver_stats.sort_values(\"total_mentions\", ascending=False)\n",
    "driver_stats.to_csv(\"output_f1_driver_mentions.csv\", index=False)\n",
    "\n",
    "\n",
    "# ---- 4.4 Analysis 3: Driver sentiment vs video engagement ----\n",
    "\n",
    "comments_with_video = comments_exploded.merge(\n",
    "    videos_merged_df[[\"videoId\", \"viewCount\", \"likeCount\", \"commentCount\"]],\n",
    "    on=\"videoId\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"_comment\", \"_video\"),\n",
    ")\n",
    "\n",
    "comments_with_video[\"like_view_ratio\"] = comments_with_video[\"likeCount_video\"] / (\n",
    "    comments_with_video[\"viewCount\"].replace(0, np.nan)\n",
    ")\n",
    "comments_with_video[\"comment_view_ratio\"] = comments_with_video[\"commentCount\"] / (\n",
    "    comments_with_video[\"viewCount\"].replace(0, np.nan)\n",
    ")\n",
    "\n",
    "driver_sentiment_engagement = (\n",
    "    comments_with_video.groupby(\"drivers_mentioned\")\n",
    "    .agg(\n",
    "        total_mentions=(\"commentId\", \"count\"),\n",
    "        positive_comments=(\"sentiment_label\", lambda x: (x == \"positive\").sum()),\n",
    "        negative_comments=(\"sentiment_label\", lambda x: (x == \"negative\").sum()),\n",
    "        neutral_comments=(\"sentiment_label\", lambda x: (x == \"neutral\").sum()),\n",
    "        avg_comment_sentiment=(\"sentiment_score\", \"mean\"),\n",
    "        avg_comment_likes=(\"likeCount_comment\", \"mean\"),\n",
    "        avg_video_like_view_ratio=(\"like_view_ratio\", \"mean\"),\n",
    "        avg_video_comment_view_ratio=(\"comment_view_ratio\", \"mean\"),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"drivers_mentioned\": \"driver_name\"})\n",
    ")\n",
    "\n",
    "driver_sentiment_engagement[\"positive_ratio\"] = (\n",
    "    driver_sentiment_engagement[\"positive_comments\"]\n",
    "    / driver_sentiment_engagement[\"total_mentions\"]\n",
    ")\n",
    "\n",
    "driver_sentiment_engagement = driver_sentiment_engagement.sort_values(\n",
    "    \"total_mentions\", ascending=False\n",
    ")\n",
    "driver_sentiment_engagement.to_csv(\n",
    "    \"output_f1_driver_sentiment_engagement.csv\", index=False\n",
    ")\n",
    "\n",
    "print(\"Analysis complete. CSV files created:\")\n",
    "print(\" - output_f1_video_summary.csv\")\n",
    "print(\" - output_f1_driver_mentions.csv\")\n",
    "print(\" - output_f1_driver_sentiment_engagement.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93012a4e-3f44-4f06-9385-1e7d909eb7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
